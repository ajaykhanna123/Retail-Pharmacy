{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0b14ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in dictionary= 6\n",
      "Dictionary is =  {'prime': 1, 'natural': 2, 'text': 3, 'the': 4, 'leader': 5, 'language': 6}\n",
      "data  {'prime': 1, 'natural': 2, 'text': 3, 'the': 4, 'leader': 5, 'language': 6}\n",
      "<function embedding_for_vocab at 0x0000024E009E84C0>\n",
      "Dense vector for first word is =>  [ 0.50795001  0.69881999  0.41468     0.49972999  0.82731003  0.58882999\n",
      " -0.43408     0.21703    -1.81809998 -0.74273998 -0.17991     0.28492999\n",
      " -0.16937     0.87449002  0.55294001  0.91030997  0.21957    -0.4851\n",
      "  0.75489002  0.52341998  0.5438      0.10108    -0.07919    -0.11478\n",
      "  0.29473999 -1.60039997  0.52854002  0.04084    -0.7198      1.93540001\n",
      "  2.81900001  0.60715997 -1.12080002  0.057194    0.14309999  0.47372001\n",
      "  0.59581     0.11381    -0.79955    -0.28086999 -0.32896999  1.32560003\n",
      " -1.18040001 -1.38600004  0.20202     0.51486999 -1.90680003  0.65419\n",
      "  1.72459996 -0.6013    ]\n"
     ]
    }
   ],
   "source": [
    "# code for Glove word embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "x = {'text', 'the', 'leader', 'prime',\n",
    "\t'natural', 'language'}\n",
    "\n",
    "# create the dict.\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(x)\n",
    "\n",
    "# number of unique words in dict.\n",
    "print(\"Number of unique words in dictionary=\",\n",
    "\tlen(tokenizer.word_index))\n",
    "print(\"Dictionary is = \", tokenizer.word_index)\n",
    "\n",
    "\n",
    "def embedding_for_vocab(filepath, word_index,\n",
    "\t\t\t\t\t\tembedding_dim):\n",
    "\tvocab_size = len(word_index) + 1\n",
    "\t\n",
    "\t# Adding again 1 because of reserved 0 index\n",
    "\tembedding_matrix_vocab = np.zeros((vocab_size,\n",
    "\t\t\t\t\t\t\t\t\tembedding_dim))\n",
    "\n",
    "\twith open(filepath, encoding=\"utf8\") as f:\n",
    "\t\tfor line in f:\n",
    "\t\t\tword, *vector = line.split()\n",
    "\t\t\tif word in word_index:\n",
    "\t\t\t\tidx = word_index[word]\n",
    "\t\t\t\tembedding_matrix_vocab[idx] = np.array(\n",
    "\t\t\t\t\tvector, dtype=np.float32)[:embedding_dim]\n",
    "\n",
    "\treturn embedding_matrix_vocab\n",
    "\n",
    "\n",
    "# matrix for vocab: word_index\n",
    "embedding_dim = 50\n",
    "path=r'C:\\Users\\Ajay\\Documents\\Code\\EXL\\Projects\\Retail Pharmacy\\Data\\glove.6B.50d.txt'\n",
    "print(\"data \",tokenizer.word_index)\n",
    "embedding_matrix_vocab = embedding_for_vocab(path, tokenizer.word_index,embedding_dim)\n",
    "print(embedding_for_vocab)\n",
    "print(\"Dense vector for first word is => \",embedding_matrix_vocab[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c11cd27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loading_embeddings():\n",
    "    \n",
    "    embeddings_index = {}\n",
    "    f = open(path, encoding=\"utf8\") # loading the file\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51826e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 100000\n",
    "def prepare_embedding_matrix(word_index):\n",
    "    \n",
    "    embeddings_index = loading_embeddings()\n",
    "    num_words = min(MAX_NB_WORDS, len(word_index))\n",
    "    embedding_matrix = np.zeros((num_words + 1, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= MAX_NB_WORDS:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix, num_words\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
