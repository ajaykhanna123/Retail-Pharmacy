{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6c20863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "import pandas as pd\n",
    "import pickle as pk\n",
    "from scipy import sparse as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8f87ed8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df =pd.read_csv(r'C:\\Users\\Ajay\\Documents\\Code\\EXL\\Projects\\Retail Pharmacy\\TextClustering\\Data\\2021VAERSDATA.csv',encoding='latin1')\n",
    "docs = array(df['SYMPTOM_TEXT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "84c3fb78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Right side of epiglottis swelled up and hinder swallowing pictures taken Benadryl Tylenol taken'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e093d72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "df=df[df['SYMPTOM_TEXT'].notna()]\n",
    "docs = array(df['SYMPTOM_TEXT'])\n",
    "def docs_preprocessor(docs):\n",
    "    try:\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        for idx in range(len(docs)):\n",
    "            \n",
    "            docs[idx] = docs[idx].lower()  # Convert to lowercase.\n",
    "            docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n",
    "\n",
    "        # Remove numbers, but not words that contain numbers.\n",
    "        docs = [[token for token in doc if not token.isdigit()] for doc in docs]\n",
    "\n",
    "        # Remove words that are only one character.\n",
    "        docs = [[token for token in doc if len(token) > 3] for doc in docs]\n",
    "\n",
    "        # Lemmatize all words in documents.\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]\n",
    "    except:\n",
    "        print(docs[idx-1],docs[idx])\n",
    "  \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4bd842e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18604\\2633787503.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m     \u001b[0mrow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'[^A-Za-z]+'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m     \u001b[0mtokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\python3-directml\\lib\\re.py\u001b[0m in \u001b[0;36msub\u001b[1;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[0ma\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mMatch\u001b[0m \u001b[0mobject\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mmust\u001b[0m \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m     a replacement string to be used.\"\"\"\n\u001b[1;32m--> 194\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msubn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "import nltk\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "# NLP text tokens\n",
    "df=df[df['SYMPTOM_TEXT'].notna()]\n",
    "def tokenize(text):\n",
    "    words=word_tokenize(text)\n",
    "    return words\n",
    "\n",
    "# removing stopwords\n",
    "stop=stopwords.words('english')\n",
    "puncs=list(string.punctuation)\n",
    "stop=stop+puncs\n",
    "def remove_stops(words):\n",
    "    cleaned=[w for w in words if not w.lower() in stop]\n",
    "    return cleaned\n",
    "# document parsing\n",
    "def pos_tg(cleaned):\n",
    "    pos=np.array(pos_tag(cleaned))\n",
    "    return pos\n",
    "\n",
    "def get_simple_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "# lemitizing\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(pos):\n",
    "    lemmatized=[]\n",
    "    for w in pos:\n",
    "        lemmatizedword=lemmatizer.lemmatize(w[0],get_simple_pos(w[1]))\n",
    "        lemmatized.append(lemmatizedword)\n",
    "    return lemmatized\n",
    "def preprocess(text):\n",
    "    words=tokenize(text)\n",
    "    cleaned=remove_stops(words)\n",
    "    pos=pos_tag(cleaned)\n",
    "    lemmatized=lemmatize(pos)\n",
    "    preprocessed=\" \".join(lemmatized)\n",
    "    return preprocessed\n",
    "\n",
    "# data preprocessing step where some characters which are not required are removed \n",
    "# 1. special characters\n",
    "# 2. digits in this use case\n",
    "# 3. extra spaces\n",
    "# 4. some words like block are used in lot of sentences and is surrounded by other characters ..needed to be separated\n",
    "# 5 words less than  3 and greater than 20 add no meaning to data , so those are also removed\n",
    "worddic={}\n",
    "rows=[]\n",
    "for i in df.index:\n",
    "    row=df['SYMPTOM_TEXT'][i]\n",
    "    try:\n",
    "        row=df['SYMPTOM_TEXT'][i].lower()\n",
    "    except:\n",
    "        print(row)\n",
    "    \n",
    "    row=re.sub('[^A-Za-z]+', ' ', row)\n",
    "    tokens=row.split(\" \")\n",
    "    \n",
    "    tokens=[t.lower() for t in tokens if len(t)>=3 and len(t)<20]\n",
    "    text=\" \".join(tokens)\n",
    "\n",
    "    row=preprocess(text)\n",
    "    tokens=row.split(\" \")\n",
    "    for j in tokens:\n",
    "        worddic[j]=worddic.get(j,0)+1\n",
    "    rows.append(row)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4b248cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = docs_preprocessor(docs)\n",
    "# docs=rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f60009f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Phrases\n",
    "# Add bigrams and trigrams to docs (only ones that appear 10 times or more).\n",
    "bigram = Phrases(docs, min_count=10)\n",
    "trigram = Phrases(bigram[docs])\n",
    "\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)\n",
    "    for token in trigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4e56073a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in initital documents: 27693\n",
      "Number of unique words after removing rare and common words: 9095\n"
     ]
    }
   ],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "dictionary = Dictionary(docs)\n",
    "print('Number of unique words in initital documents:', len(dictionary))\n",
    "\n",
    "# Filter out words that occur less than 10 documents, or more than 20% of the documents.\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.2)\n",
    "print('Number of unique words after removing rare and common words:', len(dictionary))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f7f4d2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "56b37f9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.corpora.dictionary.Dictionary at 0x1d914af1790>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "040bc66a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 9095\n",
      "Number of documents: 34119\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3bdd0590",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "160fe389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.id2token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "19761bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9min 40s\n"
     ]
    }
   ],
   "source": [
    "num_topics = 4\n",
    "chunksize = 500 # size of the doc looked at every pass\n",
    "passes = 20 # number of passes through documents\n",
    "iterations = 400\n",
    "eval_every = 1  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# Make a index to word dictionary.\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "%time model = LdaModel(corpus=corpus, id2word=id2word, chunksize=chunksize, \\\n",
    "                       alpha='auto', eta='auto', \\\n",
    "                       iterations=iterations, num_topics=num_topics, \\\n",
    "                       passes=passes, eval_every=eval_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1feac8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyLDAvis\n",
      "  Using cached pyLDAvis-3.3.1.tar.gz (1.7 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting numexpr\n",
      "  Downloading numexpr-2.8.4-cp37-cp37m-win_amd64.whl (92 kB)\n",
      "     ---------------------------------------- 92.7/92.7 kB 2.6 MB/s eta 0:00:00\n",
      "Collecting future\n",
      "  Downloading future-0.18.2.tar.gz (829 kB)\n",
      "     ------------------------------------- 829.2/829.2 kB 10.5 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: pandas>=1.2.0 in c:\\users\\ajay\\anaconda3\\envs\\python3-directml\\lib\\site-packages (from pyLDAvis) (1.3.5)\n",
      "Requirement already satisfied: joblib in c:\\users\\ajay\\anaconda3\\envs\\python3-directml\\lib\\site-packages (from pyLDAvis) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\ajay\\anaconda3\\envs\\python3-directml\\lib\\site-packages (from pyLDAvis) (1.21.6)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\ajay\\anaconda3\\envs\\python3-directml\\lib\\site-packages (from pyLDAvis) (1.0.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ajay\\anaconda3\\envs\\python3-directml\\lib\\site-packages (from pyLDAvis) (65.5.0)\n",
      "Collecting funcy\n",
      "  Using cached funcy-1.17-py2.py3-none-any.whl (33 kB)\n",
      "Requirement already satisfied: scipy in c:\\users\\ajay\\anaconda3\\envs\\python3-directml\\lib\\site-packages (from pyLDAvis) (1.7.3)\n",
      "Requirement already satisfied: sklearn in c:\\users\\ajay\\anaconda3\\envs\\python3-directml\\lib\\site-packages (from pyLDAvis) (0.0.post1)\n",
      "Requirement already satisfied: gensim in c:\\users\\ajay\\anaconda3\\envs\\python3-directml\\lib\\site-packages (from pyLDAvis) (4.2.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ajay\\anaconda3\\envs\\python3-directml\\lib\\site-packages (from pyLDAvis) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\ajay\\anaconda3\\envs\\python3-directml\\lib\\site-packages (from pandas>=1.2.0->pyLDAvis) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\ajay\\anaconda3\\envs\\python3-directml\\lib\\site-packages (from pandas>=1.2.0->pyLDAvis) (2022.6)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\ajay\\anaconda3\\envs\\python3-directml\\lib\\site-packages (from gensim->pyLDAvis) (6.2.0)\n",
      "Requirement already satisfied: Cython==0.29.28 in c:\\users\\ajay\\anaconda3\\envs\\python3-directml\\lib\\site-packages (from gensim->pyLDAvis) (0.29.28)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ajay\\anaconda3\\envs\\python3-directml\\lib\\site-packages (from jinja2->pyLDAvis) (2.1.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\ajay\\anaconda3\\envs\\python3-directml\\lib\\site-packages (from scikit-learn->pyLDAvis) (3.1.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ajay\\anaconda3\\envs\\python3-directml\\lib\\site-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->pyLDAvis) (1.16.0)\n",
      "Building wheels for collected packages: pyLDAvis, future\n",
      "  Building wheel for pyLDAvis (pyproject.toml): started\n",
      "  Building wheel for pyLDAvis (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for pyLDAvis: filename=pyLDAvis-3.3.1-py2.py3-none-any.whl size=136904 sha256=e4afecdbf2b28cf6102ed89980939edf0f10126c5617118bfc4d724460261177\n",
      "  Stored in directory: c:\\users\\ajay\\appdata\\local\\pip\\cache\\wheels\\c9\\21\\f6\\17bcf2667e8a68532ba2fbf6d5c72fdf4c7f7d9abfa4852d2f\n",
      "  Building wheel for future (setup.py): started\n",
      "  Building wheel for future (setup.py): finished with status 'done'\n",
      "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491058 sha256=707a2bb9a0cf13c4eaf74cb35b3f0999abd05b32caa63f5450ac27c6adc8f461\n",
      "  Stored in directory: c:\\users\\ajay\\appdata\\local\\pip\\cache\\wheels\\56\\b0\\fe\\4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
      "Successfully built pyLDAvis future\n",
      "Installing collected packages: funcy, numexpr, future, pyLDAvis\n",
      "Successfully installed funcy-1.17 future-0.18.2 numexpr-2.8.4 pyLDAvis-3.3.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1962a9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dbc93b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ajay\\Anaconda3\\envs\\python3-directml\\lib\\site-packages\\pyLDAvis\\_prepare.py:247: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  by='saliency', ascending=False).head(R).drop('saliency', 1)\n"
     ]
    }
   ],
   "source": [
    "p=pyLDAvis.gensim_models.prepare(model, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "da58c3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.save_html(p, 'lda.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "40bc355e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f2db881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:DML:0', device_type='DML')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "import tensorflow as tf\n",
    "tf.config.experimental.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "277e5a69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "340a89c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "devices=device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c93aac37",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.config.experimental.set_visible_devices=devices[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed7e8c28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function tensorflow.python.framework.config.get_visible_devices(device_type=None)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.compat.v1.config.experimental.get_visible_devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19e5d8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Ajay\\AppData\\Local\\Temp\\ipykernel_18604\\2269825933.py:1: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Ajay\\AppData\\Local\\Temp\\ipykernel_18604\\2269825933.py:1: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:DML:0 -> {\"name\": \"AMD Radeon(TM) Graphics\", \"vendor_id\": 4098, \"device_id\": 5708, \"driver_version\": \"30.0.13032.0\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d2cb8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69f54300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/device:DML:0'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea25e12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
